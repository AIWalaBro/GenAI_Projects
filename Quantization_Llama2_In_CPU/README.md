# Qunatization in CPU Inferencing Llama-2 from Local


# How to run
```bash
- conda create -p venv_llama2_cpu python=3.10 -y
```

```bash
- cond activate venv_llama2_cpu
```

```bash
- pip install -r requirements.txt
```
```bash
- python app.py
```


Download the quantize model from the link provided in model folder & keep the model in the model directory:

## Download the Llama 2 Model:

llama-2-7b-chat.ggmlv3.q4_0.bin

## From the following link:
https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main